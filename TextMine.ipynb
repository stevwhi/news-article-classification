{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevwhi/news-article-classification/blob/main/TextMine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Newsgroup data set:\n",
        "[Data Set](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
        "\n",
        "Text processing commandsï¼š\n",
        "\n",
        "[Tf-idf](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)\n",
        "\n",
        "[countvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n"
      ],
      "metadata": {
        "id": "1ZVVMai8xRFX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhgYu18FM-_L"
      },
      "source": [
        "from IPython import get_ipython\n",
        "get_ipython().magic('reset -sf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "934uDa63sRnp"
      },
      "source": [
        "# 1: Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxDjTTcDrtVl"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twenty_train = fetch_20newsgroups(subset='train',  categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8VrHduYC-kd"
      },
      "source": [
        "Use number to choose newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvU6i2KNHzC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38abc895-9ee7-4f1e-8c19-e77e61d8e0a5"
      },
      "source": [
        "index=input('type your number?')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type your student number?31021097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=divmod(int(index),4)\n",
        "yourdata1=x[1]\n",
        "y=divmod(int(index),3)\n",
        "yourdata2=y[1]\n",
        "\n",
        "print('This is your data set index ----> (', x[1], y[1], ')' )"
      ],
      "metadata": {
        "id": "a8Gmf-HdGcWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae1bd3a-b8cb-4a4e-a4c6-69c2e5a2faf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is your data set index ----> ( 1 2 )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXPpVRSGAPM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2abeb97-6021-435b-fa4d-6728db94b384"
      },
      "source": [
        "data1= twenty_train.target_names[x[1]]\n",
        "data2= twenty_train.target_names[y[1]]\n",
        "categories1=[data1,data2]\n",
        "print(categories1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comp.graphics', 'sci.med']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1jAHpjtaSPu"
      },
      "source": [
        "# 2: Process text data, extract features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El_vU9NocxVC"
      },
      "source": [
        "# 2.1 preprocessing example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC3yT07PJnKp"
      },
      "source": [
        "\n",
        "\n",
        "dataset=twenty_train.data[1]\n",
        "print(dataset)\n",
        "# please   replace 1 in bracket to other data sample and explore the code\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e42954-fd2c-45cd-ba0e-c23567c53333",
        "id": "cRHBOWMnAAWA"
      },
      "source": [
        "\n",
        "# tokenize: search: nltk tokenize\n",
        "example = \"This is an example sentence.\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "example_tokenize =word_tokenize(example)\n",
        "#example_tokenize= word_tokenize(dataset) # replace example in bracket to dataset.\n",
        "print(\"-------------------------tokenize:\")\n",
        "print(example_tokenize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------tokenize:\n",
            "['This', 'is', 'an', 'example', 'sentence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f48c3da4-387a-4f72-9103-26af5f40358c",
        "id": "kAaaP86_Ahmo"
      },
      "source": [
        "# stemmer: search: nltk stemmer\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "example_stem = stemmer.stem(example)  # replace .....\n",
        "print(\"-------------------------stem:\")\n",
        "print(example_stem)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------stem:\n",
            "this is an example sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pos_taging: search: nltk pos tagging example\n",
        "example_posTag=nltk.pos_tag(example_tokenize)\n",
        "print(\"-------------------------pos_taging:\")\n",
        "print(example_posTag)"
      ],
      "metadata": {
        "id": "iL4Vr-m0ApLu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d30920-db6d-40cc-89ee-1b237771d060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------pos_taging:\n",
            "[('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN'), ('sentence', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51277cb6-bc61-467c-f9f4-3156aac371d0",
        "id": "O-n6AAbc_-_5"
      },
      "source": [
        " # consituency parsing, chunking\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "result = cp.parse(example_posTag)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S This/DT is/VBZ (NP an/DT example/NN) (NP sentence/NN) ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7VpnVNpKuUt"
      },
      "source": [
        "#2.2 Actual NLP Preprocesssing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8u5y9adK3tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca62cec-ce6c-45ec-8db0-6c036db8355c"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "stopwordEn = stopwords.words('english')\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "\n",
        "def lemmaWord(word):\n",
        "    lemma = wordnet.morphy(word)\n",
        "    if lemma is not None:\n",
        "        return lemma\n",
        "    else:\n",
        "        return word\n",
        "\n",
        "def stemWord(word):\n",
        "    stem = stemmer.stem(word)\n",
        "    if stem is not None:\n",
        "        return stem\n",
        "    else:\n",
        "        return word\n",
        "\n",
        "def processText(text,lemma=False, gram=1, rmStop=True): # default remove stop words\n",
        "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b|@\\w+|#', '', text, flags=re.MULTILINE) #delete URL, #hashtag# , and @xxx\n",
        "    tokens = word_tokenize(text)\n",
        "    whitelist = [\"n't\", \"not\", \"no\"]\n",
        "    new_tokens = []\n",
        "    stoplist = stopwordEn if rmStop else []\n",
        "    for i in tokens:\n",
        "      i = i.lower()\n",
        "      if i.isalpha() and (i not in stoplist or i in whitelist):  #i not in ['.',',',';']  and (...)\n",
        "        if lemma: i = lemmaWord(i)\n",
        "        new_tokens.append(i)\n",
        "    del tokens\n",
        "    # tokens = [lemmaWord(i.lower()) if lemma else i.lower() for i in tokens if (i.lower() not in stoplist or i.lower() in whitelist) and i.isalpha()]\n",
        "    if gram<=1:\n",
        "        return new_tokens\n",
        "    else:\n",
        "        return [' '.join(i) for i in nltk.ngrams(new_tokens, gram)]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKFoZaWSVrnq"
      },
      "source": [
        "def getTags(text):\n",
        "  token = word_tokenize(text)\n",
        "  token = [l.lower() for l in token]\n",
        "  train_tags = nltk.pos_tag(token)\n",
        "  return [i[1] for i in train_tags]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8mwYOcFcS02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b5ae58-9ea9-4e4e-ccf1-2b020307e0ef"
      },
      "source": [
        "print(processText(dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aniruddha', 'deglurkar', 'subject', 'help', 'splitting', 'trimming', 'region', 'along', 'mesh', 'organization', 'university', 'kentucky', 'dept', 'math', 'sciences', 'lines', 'hi', 'problem', 'hope', 'help', 'solve', 'background', 'problem', 'rectangular', 'mesh', 'uv', 'domain', 'mesh', 'mapping', 'bezier', 'patch', 'area', 'domain', 'inside', 'trimming', 'loop', 'rendered', 'trimming', 'loop', 'set', 'bezier', 'curve', 'segments', 'sake', 'notation', 'mesh', 'made', 'cells', 'problem', 'trimming', 'area', 'split', 'individual', 'smaller', 'cells', 'bounded', 'trimming', 'curve', 'segments', 'cell', 'wholly', 'inside', 'area', 'output', 'whole', 'else', 'trivially', 'rejected', 'body', 'know', 'thiss', 'done', 'algo', 'somewhere', 'help', 'would', 'appreciated', 'thanks', 'ani', 'get', 'irritated', 'human', 'stay', 'cool', 'divine']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZRqySa8cbr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6580a66-b9bb-4622-9a9b-16bf2c14169f"
      },
      "source": [
        "print(getTags(dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['IN', ':', 'NN', 'NN', 'NN', '(', 'JJ', 'NN', 'NN', ')', 'NN', ':', 'NN', ':', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'NN', 'IN', 'NN', ',', 'NN', '.', 'IN', 'NN', 'NNS', 'NNS', ':', 'CD', 'NN', ',', 'NN', 'VBP', 'DT', 'NN', ',', 'NN', 'VBP', 'DT', 'IN', 'DT', 'NNP', 'POS', 'MD', 'VB', 'PRP', 'VB', '.', 'NN', 'IN', 'DT', 'NN', ':', 'NN', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'VBP', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'CD', 'NN', 'NN', 'IN', 'CD', '.', 'DT', 'NN', 'IN', 'DT', 'NN', 'WDT', 'VBZ', 'IN', 'DT', 'NN', 'NN', 'VBD', 'TO', 'VB', 'VBN', '.', 'DT', 'VBG', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'NN', 'NNS', '.', 'IN', 'DT', 'NN', 'IN', 'NN', ':', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'IN', 'NNS', '.', 'PRP$', 'NN', 'VBZ', 'DT', ':', 'DT', 'VBG', 'NN', 'VBZ', 'TO', 'VB', 'VBN', 'RP', 'IN', 'JJ', 'JJR', 'NNS', 'VBN', 'IN', 'DT', 'VBG', 'NN', 'NNS', '.', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'IN', 'DT', 'NN', ':', 'RB', 'PRP', 'VBZ', 'NN', 'IN', 'DT', 'NN', ',', 'VB', 'PRP', 'VBZ', 'RB', 'VBN', '.', 'VBZ', 'DT', 'NN', 'VB', 'WRB', 'JJ', 'MD', 'VB', 'VBN', ',', 'CC', 'VBZ', 'RB', 'DT', 'NN', '.', 'RB', 'IN', 'VBG', 'DT', '.', 'DT', 'NN', 'MD', 'VB', 'VBN', '.', 'NNS', ',', 'NN', '.', ':', 'TO', 'VB', 'JJ', 'VBZ', 'JJ', ',', 'TO', 'VB', 'JJ', ',', 'NN', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44xTvpLa_UC9"
      },
      "source": [
        "# Step 3: Pipeline build"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9HMKvgGMHPB",
        "outputId": "3fc7cc48-6e6d-40c0-f64d-a8208ff40841"
      },
      "source": [
        "print(categories1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comp.graphics', 'sci.med']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PDFkEEiL1GQ"
      },
      "source": [
        "twenty_train1 = fetch_20newsgroups(subset='train',  categories=categories1, shuffle=True, random_state=42)\n",
        "twenty_test1 = fetch_20newsgroups(subset='test',  categories=categories1, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNm3axlhdzlF"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "\n",
        "# Level: lexicon, model: tf-idf\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer=processText)),\n",
        "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "\n",
        "\n",
        "    #('clf', SGDClassifier())\n",
        "    ('clf', LogisticRegression())\n",
        "\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vuq37Bf3Qjpn"
      },
      "source": [
        "# train the model\n",
        "text_clf.fit(twenty_train1.data, twenty_train1.target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjQ8DmPNRUuJ"
      },
      "source": [
        "# Step 4: Make Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMdoIHjMRWce"
      },
      "source": [
        "# prediction with dev/test set\n",
        "predicted = text_clf.predict(twenty_test1.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GXHJHqoBmyJ"
      },
      "source": [
        "# Step 5: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdB9js0QDErf"
      },
      "source": [
        "# evaluate prediction on dev set\n",
        "from sklearn import metrics\n",
        "print(\"Accuracy:\", metrics.accuracy_score(twenty_test1.target, predicted))\n",
        "\n",
        "print(metrics.classification_report(twenty_test1.target, predicted, target_names=twenty_test1.target_names))\n",
        "\n",
        "# confusion class\n",
        "pd.DataFrame(metrics.confusion_matrix(twenty_test1.target, predicted),\n",
        "             columns=twenty_test1.target_names,index=twenty_test1.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisation"
      ],
      "metadata": {
        "id": "Kem1fI1lchKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "accuracy = metrics.accuracy_score(twenty_test1.target, predicted)\n",
        "report = metrics.classification_report(twenty_test1.target, predicted, target_names=twenty_test1.target_names, output_dict=True)\n",
        "\n",
        "# convert to dataframe\n",
        "report_df = pd.DataFrame(report).drop(['accuracy', 'macro avg', 'weighted avg'], errors='ignore').transpose()\n",
        "report_df.drop(columns=['support'], inplace=True)\n",
        "\n",
        "melted_report_df = report_df.reset_index().melt(id_vars='index', value_vars=['precision', 'recall', 'f1-score'])\n",
        "melted_report_df.rename(columns={'index': 'Category', 'variable': 'Score Type', 'value': 'Score'}, inplace=True)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.barplot(x='Category', y='Score', hue='Score Type', data=melted_report_df)\n",
        "plt.title('Precision, Recall, and F1-Scores by Category')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylim(melted_report_df['Score'].min() - 0.05, 1)  # Adjust the y-axis to not start from zero\n",
        "plt.ylabel('Scores')\n",
        "plt.xlabel('Categories')\n",
        "plt.legend(title='Score Type')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "conf_mat = metrics.confusion_matrix(twenty_test1.target, predicted)\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=twenty_test1.target_names, yticklabels=twenty_test1.target_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8gZ25SMxZPoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCLCqFXPQsRq"
      },
      "source": [
        "# Step 6: Error Analysis\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvBw9qkKDS-m"
      },
      "source": [
        "df_pred = pd.DataFrame({'news':twenty_test1.data,'prediction':predicted, 'true':twenty_test1.target})\n",
        "df_pred[df_pred['true'] != df_pred['prediction']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KEYWORD ANALYSIS"
      ],
      "metadata": {
        "id": "MuUb1Ji2gpmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top 10 for misclassified"
      ],
      "metadata": {
        "id": "YNFD9dD0gmCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# filter\n",
        "misclassified = df_pred[df_pred['prediction'] != df_pred['true']]\n",
        "\n",
        "# extract keywords\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(misclassified['news'])\n",
        "\n",
        "# Sum occurences\n",
        "sum_words = X.sum(axis=0)\n",
        "\n",
        "# dictionary\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
        "\n",
        "# sort by freq.\n",
        "sorted_words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# filter top 10\n",
        "top_words_freq = sorted_words_freq[:10]\n",
        "for word, freq in top_words_freq:\n",
        "    print(f'{word}: {freq}')\n",
        "\n",
        "# Print the first misclassified example\n",
        "first_misclassified = misclassified.iloc[0]\n",
        "print(\"Misclassified Text:\\n\", first_misclassified['news'])\n",
        "print(\"Predicted Label:\", first_misclassified['prediction'])\n",
        "print(\"True Label:\", first_misclassified['true'])\n"
      ],
      "metadata": {
        "id": "n-pXy-cHwsEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top 10 for each newsgroup"
      ],
      "metadata": {
        "id": "i59Nb5zwgiZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Define the function to get top n words from a corpus\n",
        "def get_top_n_words(corpus, n=10):\n",
        "    vec = CountVectorizer(stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Dataset\n",
        "categories = ['comp.graphics', 'sci.med']\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "# Filter\n",
        "comp_graphics_texts = [text for text, label in zip(twenty_train.data, twenty_train.target) if twenty_train.target_names[label] == 'comp.graphics']\n",
        "sci_med_texts = [text for text, label in zip(twenty_train.data, twenty_train.target) if twenty_train.target_names[label] == 'sci.med']\n",
        "\n",
        "# 10 for each category\n",
        "top_words_comp_graphics = get_top_n_words(comp_graphics_texts, 10)\n",
        "top_words_sci_med = get_top_n_words(sci_med_texts, 10)\n",
        "\n",
        "\n",
        "print(\"Top words for 'comp.graphics':\", top_words_comp_graphics)\n",
        "print(\"Top words for 'sci.med':\", top_words_sci_med)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gBRaRzwYlhs",
        "outputId": "46283ded-5b7c-4209-bf57-5bbd16559a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top words for 'comp.graphics': [('edu', 1037), ('lines', 645), ('subject', 625), ('graphics', 604), ('organization', 575), ('image', 532), ('com', 426), ('university', 359), ('posting', 307), ('file', 300)]\n",
            "Top words for 'sci.med': [('edu', 1388), ('com', 853), ('subject', 636), ('organization', 606), ('lines', 592), ('article', 485), ('writes', 439), ('cs', 374), ('pitt', 341), ('people', 321)]\n"
          ]
        }
      ]
    }
  ]
}